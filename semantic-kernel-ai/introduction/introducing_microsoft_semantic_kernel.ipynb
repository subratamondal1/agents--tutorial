{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Kernel can **`automatically orchestrate` different plugins by using a `planner`**. With the planner, a user can ask your application to achieve a complex goal. For example, if you have a function that identifies which animal is in a picture and another function that tells knock-knock jokes, your user can say, “Tell me a knock-knock joke about the animal in the picture in this URL,” and the planner will automatically understand that it needs to call the identification function first and the “tell joke” function after it. **Semantic Kernel will automatically search and combine your plugins to achieve that goal and create a plan.** Then, Semantic Kernel will execute that plan and provide a response to the user:\n",
    "\n",
    "<img src=\"../assets/semantic-kernel-architecture.png\" />\n",
    "\n",
    "\n",
    "**`LLM Cascade:`** The process of sending simpler requests to simpler models and complex requests to more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Kernel in module semantic_kernel.kernel object:\n",
      "\n",
      "class Kernel(semantic_kernel.filters.kernel_filters_extension.KernelFilterExtension, semantic_kernel.functions.kernel_function_extension.KernelFunctionExtension, semantic_kernel.services.kernel_services_extension.KernelServicesExtension, semantic_kernel.reliability.kernel_reliability_extension.KernelReliabilityExtension)\n",
      " |  Kernel(plugins: semantic_kernel.functions.kernel_plugin.KernelPlugin | dict[str, semantic_kernel.functions.kernel_plugin.KernelPlugin] | list[semantic_kernel.functions.kernel_plugin.KernelPlugin] | None = None, services: Union[~AI_SERVICE_CLIENT_TYPE, list[~AI_SERVICE_CLIENT_TYPE], dict[str, ~AI_SERVICE_CLIENT_TYPE], NoneType] = None, ai_service_selector: semantic_kernel.services.ai_service_selector.AIServiceSelector | None = None, *, retry_mechanism: semantic_kernel.reliability.retry_mechanism_base.RetryMechanismBase = None, function_invocation_filters: list[tuple[int, collections.abc.Callable[[~FILTER_CONTEXT_TYPE, collections.abc.Callable[[~FILTER_CONTEXT_TYPE], None]], None]]] = None, prompt_rendering_filters: list[tuple[int, collections.abc.Callable[[~FILTER_CONTEXT_TYPE, collections.abc.Callable[[~FILTER_CONTEXT_TYPE], None]], None]]] = None, auto_function_invocation_filters: list[tuple[int, collections.abc.Callable[[~FILTER_CONTEXT_TYPE, collections.abc.Callable[[~FILTER_CONTEXT_TYPE], None]], None]]] = None) -> None\n",
      " |\n",
      " |  The Kernel of Semantic Kernel.\n",
      " |\n",
      " |  This is the main entry point for Semantic Kernel. It provides the ability to run\n",
      " |  functions and manage filters, plugins, and AI services.\n",
      " |\n",
      " |  Attributes:\n",
      " |      function_invocation_filters: Filters applied during function invocation, from KernelFilterExtension.\n",
      " |      prompt_rendering_filters: Filters applied during prompt rendering, from KernelFilterExtension.\n",
      " |      auto_function_invocation_filters: Filters applied during auto function invocation, from KernelFilterExtension.\n",
      " |      plugins: A dict with the plugins registered with the Kernel, from KernelFunctionExtension.\n",
      " |      services: A dict with the services registered with the Kernel, from KernelServicesExtension.\n",
      " |      ai_service_selector: The AI service selector to be used by the kernel, from KernelServicesExtension.\n",
      " |      retry_mechanism: The retry mechanism to be used by the kernel, from KernelReliabilityExtension.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      Kernel\n",
      " |      semantic_kernel.filters.kernel_filters_extension.KernelFilterExtension\n",
      " |      semantic_kernel.functions.kernel_function_extension.KernelFunctionExtension\n",
      " |      semantic_kernel.services.kernel_services_extension.KernelServicesExtension\n",
      " |      semantic_kernel.reliability.kernel_reliability_extension.KernelReliabilityExtension\n",
      " |      semantic_kernel.kernel_pydantic.KernelBaseModel\n",
      " |      pydantic.main.BaseModel\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, plugins: semantic_kernel.functions.kernel_plugin.KernelPlugin | dict[str, semantic_kernel.functions.kernel_plugin.KernelPlugin] | list[semantic_kernel.functions.kernel_plugin.KernelPlugin] | None = None, services: Union[~AI_SERVICE_CLIENT_TYPE, list[~AI_SERVICE_CLIENT_TYPE], dict[str, ~AI_SERVICE_CLIENT_TYPE], NoneType] = None, ai_service_selector: semantic_kernel.services.ai_service_selector.AIServiceSelector | None = None, **kwargs: Any) -> None\n",
      " |      Initialize a new instance of the Kernel class.\n",
      " |\n",
      " |      Args:\n",
      " |          plugins: The plugins to be used by the kernel, will be rewritten to a dict with plugin name as key\n",
      " |          services: The services to be used by the kernel, will be rewritten to a dict with service_id as key\n",
      " |          ai_service_selector: The AI service selector to be used by the kernel,\n",
      " |              default is based on order of execution settings.\n",
      " |          **kwargs: Additional fields to be passed to the Kernel model,\n",
      " |              these are limited to retry_mechanism and function_invoking_handlers\n",
      " |              and function_invoked_handlers, the best way to add function_invoking_handlers\n",
      " |              and function_invoked_handlers is to use the add_function_invoking_handler\n",
      " |              and add_function_invoked_handler methods.\n",
      " |\n",
      " |  async add_embedding_to_object(self, inputs: Union[~TDataModel, collections.abc.Sequence[~TDataModel]], field_to_embed: str, field_to_store: str, execution_settings: dict[str, 'PromptExecutionSettings'], container_mode: bool = False, cast_function: collections.abc.Callable[[list[float]], typing.Any] | None = None, **kwargs: Any)\n",
      " |      Gather all fields to embed, batch the embedding generation and store.\n",
      " |\n",
      " |  async invoke(self, function: 'KernelFunction | None' = None, arguments: semantic_kernel.functions.kernel_arguments.KernelArguments | None = None, function_name: str | None = None, plugin_name: str | None = None, metadata: dict[str, typing.Any] = {}, **kwargs: Any) -> semantic_kernel.functions.function_result.FunctionResult | None\n",
      " |      Execute a function and return the FunctionResult.\n",
      " |\n",
      " |      Args:\n",
      " |          function (KernelFunction): The function or functions to execute,\n",
      " |              this value has precedence when supplying both this and using function_name and plugin_name,\n",
      " |              if this is none, function_name and plugin_name are used and cannot be None.\n",
      " |          arguments (KernelArguments): The arguments to pass to the function(s), optional\n",
      " |          function_name (str | None): The name of the function to execute\n",
      " |          plugin_name (str | None): The name of the plugin to execute\n",
      " |          metadata (dict[str, Any]): The metadata to pass to the function(s)\n",
      " |          kwargs (dict[str, Any]): arguments that can be used instead of supplying KernelArguments\n",
      " |\n",
      " |      Raises:\n",
      " |          KernelInvokeException: If an error occurs during function invocation\n",
      " |\n",
      " |  async invoke_function_call(self, function_call: semantic_kernel.contents.function_call_content.FunctionCallContent, chat_history: semantic_kernel.contents.chat_history.ChatHistory, arguments: 'KernelArguments | None' = None, function_call_count: int | None = None, request_index: int | None = None, function_behavior: 'FunctionChoiceBehavior' = None) -> 'AutoFunctionInvocationContext | None'\n",
      " |      Processes the provided FunctionCallContent and updates the chat history.\n",
      " |\n",
      " |  async invoke_prompt(self, prompt: str, function_name: str | None = None, plugin_name: str | None = None, arguments: semantic_kernel.functions.kernel_arguments.KernelArguments | None = None, template_format: Literal['semantic-kernel', 'handlebars', 'jinja2'] = 'semantic-kernel', **kwargs: Any) -> semantic_kernel.functions.function_result.FunctionResult | None\n",
      " |      Invoke a function from the provided prompt.\n",
      " |\n",
      " |      Args:\n",
      " |          prompt (str): The prompt to use\n",
      " |          function_name (str): The name of the function, optional\n",
      " |          plugin_name (str): The name of the plugin, optional\n",
      " |          arguments (KernelArguments | None): The arguments to pass to the function(s), optional\n",
      " |          template_format (str | None): The format of the prompt template\n",
      " |          kwargs (dict[str, Any]): arguments that can be used instead of supplying KernelArguments\n",
      " |\n",
      " |      Returns:\n",
      " |          FunctionResult | list[FunctionResult] | None: The result of the function(s)\n",
      " |\n",
      " |  async invoke_prompt_stream(self, prompt: str, function_name: str | None = None, plugin_name: str | None = None, arguments: semantic_kernel.functions.kernel_arguments.KernelArguments | None = None, template_format: Literal['semantic-kernel', 'handlebars', 'jinja2'] = 'semantic-kernel', return_function_results: bool | None = False, **kwargs: Any) -> collections.abc.AsyncIterable[list['StreamingContentMixin'] | semantic_kernel.functions.function_result.FunctionResult | list[semantic_kernel.functions.function_result.FunctionResult]]\n",
      " |      Invoke a function from the provided prompt and stream the results.\n",
      " |\n",
      " |      Args:\n",
      " |          prompt (str): The prompt to use\n",
      " |          function_name (str): The name of the function, optional\n",
      " |          plugin_name (str): The name of the plugin, optional\n",
      " |          arguments (KernelArguments | None): The arguments to pass to the function(s), optional\n",
      " |          template_format (str | None): The format of the prompt template\n",
      " |          return_function_results (bool): If True, the function results are yielded as a list[FunctionResult]\n",
      " |          kwargs (dict[str, Any]): arguments that can be used instead of supplying KernelArguments\n",
      " |\n",
      " |      Returns:\n",
      " |          AsyncIterable[StreamingContentMixin]: The content of the stream of the last function provided.\n",
      " |\n",
      " |  async invoke_stream(self, function: 'KernelFunction | None' = None, arguments: semantic_kernel.functions.kernel_arguments.KernelArguments | None = None, function_name: str | None = None, plugin_name: str | None = None, metadata: dict[str, typing.Any] = {}, return_function_results: bool = False, **kwargs: Any) -> collections.abc.AsyncGenerator[list['StreamingContentMixin'] | semantic_kernel.functions.function_result.FunctionResult | list[semantic_kernel.functions.function_result.FunctionResult], typing.Any]\n",
      " |      Execute one or more stream functions.\n",
      " |\n",
      " |      This will execute the functions in the order they are provided, if a list of functions is provided.\n",
      " |      When multiple functions are provided only the last one is streamed, the rest is executed as a pipeline.\n",
      " |\n",
      " |      Args:\n",
      " |          function (KernelFunction): The function to execute,\n",
      " |              this value has precedence when supplying both this and using function_name and plugin_name,\n",
      " |              if this is none, function_name and plugin_name are used and cannot be None.\n",
      " |          arguments (KernelArguments | None): The arguments to pass to the function(s), optional\n",
      " |          function_name (str | None): The name of the function to execute\n",
      " |          plugin_name (str | None): The name of the plugin to execute\n",
      " |          metadata (dict[str, Any]): The metadata to pass to the function(s)\n",
      " |          return_function_results (bool): If True, the function results are yielded as a list[FunctionResult]\n",
      " |          in addition to the streaming content, otherwise only the streaming content is yielded.\n",
      " |          kwargs (dict[str, Any]): arguments that can be used instead of supplying KernelArguments\n",
      " |\n",
      " |      Yields:\n",
      " |          StreamingContentMixin: The content of the stream of the last function provided.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {}\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __private_attributes__ = {}\n",
      " |\n",
      " |  __pydantic_complete__ = True\n",
      " |\n",
      " |  __pydantic_core_schema__ = {'definitions': [{'cls': <class 'semantic_k...\n",
      " |\n",
      " |  __pydantic_custom_init__ = True\n",
      " |\n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |\n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |\n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |\n",
      " |  __pydantic_post_init__ = None\n",
      " |\n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |\n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"Kernel\", validator=Mod...\n",
      " |\n",
      " |  __signature__ = <Signature (plugins: semantic_kernel.functions.k...R_C...\n",
      " |\n",
      " |  model_computed_fields = {}\n",
      " |\n",
      " |  model_config = {'arbitrary_types_allowed': True, 'populate_by_name': T...\n",
      " |\n",
      " |  model_fields = {'ai_service_selector': FieldInfo(annotation=AIServiceS...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from semantic_kernel.filters.kernel_filters_extension.KernelFilterExtension:\n",
      " |\n",
      " |  add_filter(self, filter_type: Union[Literal[<FilterTypes.AUTO_FUNCTION_INVOCATION: 'auto_function_invocation'>, <FilterTypes.FUNCTION_INVOCATION: 'function_invocation'>, <FilterTypes.PROMPT_RENDERING: 'prompt_rendering'>], semantic_kernel.filters.filter_types.FilterTypes], filter: collections.abc.Callable[[~FILTER_CONTEXT_TYPE, collections.abc.Callable[[~FILTER_CONTEXT_TYPE], None]], None]) -> None\n",
      " |      Add a filter to the Kernel.\n",
      " |\n",
      " |      Each filter is added to the beginning of the list of filters,\n",
      " |      this is because the filters are executed in the order they are added,\n",
      " |      so the first filter added, will be the first to be executed,\n",
      " |      but it will also be the last executed for the part after `await next(context)`.\n",
      " |\n",
      " |      Args:\n",
      " |          filter_type (str): The type of the filter to add (function_invocation, prompt_rendering)\n",
      " |          filter (object): The filter to add\n",
      " |\n",
      " |      Raises:\n",
      " |          FilterDefinitionException: If an error occurs while adding the filter to the kernel\n",
      " |\n",
      " |  construct_call_stack(self, filter_type: semantic_kernel.filters.filter_types.FilterTypes, inner_function: collections.abc.Callable[[~FILTER_CONTEXT_TYPE], collections.abc.Coroutine[typing.Any, typing.Any, None]]) -> collections.abc.Callable[[~FILTER_CONTEXT_TYPE], collections.abc.Coroutine[typing.Any, typing.Any, None]]\n",
      " |      Construct the call stack for the given filter type.\n",
      " |\n",
      " |  filter(self, filter_type: Union[Literal[<FilterTypes.AUTO_FUNCTION_INVOCATION: 'auto_function_invocation'>, <FilterTypes.FUNCTION_INVOCATION: 'function_invocation'>, <FilterTypes.PROMPT_RENDERING: 'prompt_rendering'>], semantic_kernel.filters.filter_types.FilterTypes]) -> collections.abc.Callable[[collections.abc.Callable[[~FILTER_CONTEXT_TYPE, collections.abc.Callable[[~FILTER_CONTEXT_TYPE], None]], None]], collections.abc.Callable[[~FILTER_CONTEXT_TYPE, collections.abc.Callable[[~FILTER_CONTEXT_TYPE], None]], None]]\n",
      " |      Decorator to add a filter to the Kernel.\n",
      " |\n",
      " |  remove_filter(self, filter_type: Union[Literal[<FilterTypes.AUTO_FUNCTION_INVOCATION: 'auto_function_invocation'>, <FilterTypes.FUNCTION_INVOCATION: 'function_invocation'>, <FilterTypes.PROMPT_RENDERING: 'prompt_rendering'>], semantic_kernel.filters.filter_types.FilterTypes, NoneType] = None, filter_id: int | None = None, position: int | None = None) -> None\n",
      " |      Remove a filter from the Kernel.\n",
      " |\n",
      " |      Args:\n",
      " |          filter_type (str | FilterTypes | None):\n",
      " |              The type of the filter to remove.\n",
      " |          filter_id (int): The id of the hook to remove\n",
      " |          position (int): The position of the filter in the list\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from semantic_kernel.functions.kernel_function_extension.KernelFunctionExtension:\n",
      " |\n",
      " |  add_function(self, plugin_name: str, function: 'KERNEL_FUNCTION_TYPE | None' = None, function_name: str | None = None, description: str | None = None, prompt: str | None = None, prompt_template_config: semantic_kernel.prompt_template.prompt_template_config.PromptTemplateConfig | None = None, prompt_execution_settings: semantic_kernel.connectors.ai.prompt_execution_settings.PromptExecutionSettings | list[semantic_kernel.connectors.ai.prompt_execution_settings.PromptExecutionSettings] | dict[str, semantic_kernel.connectors.ai.prompt_execution_settings.PromptExecutionSettings] | None = None, template_format: Literal['semantic-kernel', 'handlebars', 'jinja2'] = 'semantic-kernel', prompt_template: semantic_kernel.prompt_template.prompt_template_base.PromptTemplateBase | None = None, return_plugin: bool = False, **kwargs: Any) -> 'KernelFunction | KernelPlugin'\n",
      " |      Adds a function to the specified plugin.\n",
      " |\n",
      " |      Args:\n",
      " |          plugin_name (str): The name of the plugin to add the function to\n",
      " |          function (KernelFunction | Callable[..., Any]): The function to add\n",
      " |          function_name (str): The name of the function\n",
      " |          plugin_name (str): The name of the plugin\n",
      " |          description (str | None): The description of the function\n",
      " |          prompt (str | None): The prompt template.\n",
      " |          prompt_template_config (PromptTemplateConfig | None): The prompt template configuration\n",
      " |          prompt_execution_settings: The execution settings, will be parsed into a dict.\n",
      " |          template_format (str | None): The format of the prompt template\n",
      " |          prompt_template (PromptTemplateBase | None): The prompt template\n",
      " |          return_plugin (bool): If True, the plugin is returned instead of the function\n",
      " |          kwargs (Any): Additional arguments\n",
      " |\n",
      " |      Returns:\n",
      " |          KernelFunction | KernelPlugin: The function that was added, or the plugin if return_plugin is True\n",
      " |\n",
      " |  add_functions(self, plugin_name: str, functions: 'list[KERNEL_FUNCTION_TYPE] | dict[str, KERNEL_FUNCTION_TYPE]') -> 'KernelPlugin'\n",
      " |      Adds a list of functions to the specified plugin.\n",
      " |\n",
      " |      Args:\n",
      " |          plugin_name (str): The name of the plugin to add the functions to\n",
      " |          functions (list[KernelFunction] | dict[str, KernelFunction]): The functions to add\n",
      " |\n",
      " |      Returns:\n",
      " |          KernelPlugin: The plugin that the functions were added to.\n",
      " |\n",
      " |  add_plugin(self, plugin: semantic_kernel.functions.kernel_plugin.KernelPlugin | object | dict[str, typing.Any] | None = None, plugin_name: str | None = None, parent_directory: str | None = None, description: str | None = None, class_init_arguments: dict[str, dict[str, typing.Any]] | None = None) -> 'KernelPlugin'\n",
      " |      Adds a plugin to the kernel's collection of plugins.\n",
      " |\n",
      " |      If a plugin is provided, it uses that instance instead of creating a new KernelPlugin.\n",
      " |      See KernelPlugin.from_directory for more details on how the directory is parsed.\n",
      " |\n",
      " |      Args:\n",
      " |          plugin: The plugin to add.\n",
      " |              This can be a KernelPlugin, in which case it is added straightaway and other parameters are ignored,\n",
      " |              a custom class that contains methods with the kernel_function decorator\n",
      " |              or a dictionary of functions with the kernel_function decorator for one or\n",
      " |              several methods.\n",
      " |          plugin_name: The name of the plugin, used if the plugin is not a KernelPlugin,\n",
      " |              if the plugin is None and the parent_directory is set,\n",
      " |              KernelPlugin.from_directory is called with those parameters,\n",
      " |              see `KernelPlugin.from_directory` for details.\n",
      " |          parent_directory: The parent directory path where the plugin directory resides\n",
      " |          description: The description of the plugin, used if the plugin is not a KernelPlugin.\n",
      " |          class_init_arguments: The class initialization arguments\n",
      " |\n",
      " |      Returns:\n",
      " |          KernelPlugin: The plugin that was added.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If a KernelPlugin needs to be created, but it is not valid.\n",
      " |\n",
      " |  async add_plugin_from_openai(self, plugin_name: str, plugin_url: str | None = None, plugin_str: str | None = None, execution_parameters: 'OpenAIFunctionExecutionParameters | None' = None, description: str | None = None) -> semantic_kernel.functions.kernel_plugin.KernelPlugin\n",
      " |      Add a plugin from an OpenAPI document.\n",
      " |\n",
      " |      Args:\n",
      " |          plugin_name (str): The name of the plugin\n",
      " |          plugin_url (str | None): The URL of the plugin\n",
      " |          plugin_str (str | None): The JSON string of the plugin\n",
      " |          execution_parameters (OpenAIFunctionExecutionParameters | None): The execution parameters\n",
      " |          description (str | None): The description of the plugin\n",
      " |\n",
      " |      Returns:\n",
      " |          KernelPlugin: The imported plugin\n",
      " |\n",
      " |      Raises:\n",
      " |          PluginInitializationError: if the plugin URL or plugin JSON/YAML is not provided\n",
      " |\n",
      " |  add_plugin_from_openapi(self, plugin_name: str, openapi_document_path: str | None = None, openapi_parsed_spec: dict[str, typing.Any] | None = None, execution_settings: 'OpenAPIFunctionExecutionParameters | None' = None, description: str | None = None) -> semantic_kernel.functions.kernel_plugin.KernelPlugin\n",
      " |      Add a plugin from the OpenAPI manifest.\n",
      " |\n",
      " |      Args:\n",
      " |          plugin_name: The name of the plugin\n",
      " |          openapi_document_path: The path to the OpenAPI document\n",
      " |          openapi_parsed_spec: The parsed OpenAPI spec\n",
      " |          execution_settings: The execution parameters\n",
      " |          description: The description of the plugin\n",
      " |\n",
      " |      Returns:\n",
      " |          KernelPlugin: The imported plugin\n",
      " |\n",
      " |      Raises:\n",
      " |          PluginInitializationError: if the plugin URL or plugin JSON/YAML is not provided\n",
      " |\n",
      " |  add_plugins(self, plugins: list[semantic_kernel.functions.kernel_plugin.KernelPlugin] | dict[str, semantic_kernel.functions.kernel_plugin.KernelPlugin | object]) -> None\n",
      " |      Adds a list of plugins to the kernel's collection of plugins.\n",
      " |\n",
      " |      Args:\n",
      " |          plugins (list[KernelPlugin] | dict[str, KernelPlugin]): The plugins to add to the kernel\n",
      " |\n",
      " |  get_full_list_of_function_metadata(self) -> list['KernelFunctionMetadata']\n",
      " |      Get a list of all function metadata in the plugins.\n",
      " |\n",
      " |  get_function(self, plugin_name: str | None, function_name: str) -> 'KernelFunction'\n",
      " |      Get a function by plugin_name and function_name.\n",
      " |\n",
      " |      Args:\n",
      " |          plugin_name (str | None): The name of the plugin\n",
      " |          function_name (str): The name of the function\n",
      " |\n",
      " |      Returns:\n",
      " |          KernelFunction: The function\n",
      " |\n",
      " |      Raises:\n",
      " |          KernelPluginNotFoundError: If the plugin is not found\n",
      " |          KernelFunctionNotFoundError: If the function is not found\n",
      " |\n",
      " |  get_function_from_fully_qualified_function_name(self, fully_qualified_function_name: str) -> 'KernelFunction'\n",
      " |      Get a function by its fully qualified name (<plugin_name>-<function_name>).\n",
      " |\n",
      " |      Args:\n",
      " |          fully_qualified_function_name (str): The fully qualified name of the function,\n",
      " |              if there is no '-' in the name, it is assumed that it is only a function_name.\n",
      " |\n",
      " |      Returns:\n",
      " |          KernelFunction: The function\n",
      " |\n",
      " |      Raises:\n",
      " |          KernelPluginNotFoundError: If the plugin is not found\n",
      " |          KernelFunctionNotFoundError: If the function is not found\n",
      " |\n",
      " |  get_list_of_function_metadata(self, *args: Any, **kwargs: Any) -> list['KernelFunctionMetadata']\n",
      " |      Get a list of all function metadata in the plugin collection.\n",
      " |\n",
      " |  get_list_of_function_metadata_bool(self, include_prompt: bool = True, include_native: bool = True) -> list['KernelFunctionMetadata']\n",
      " |      Get a list of the function metadata in the plugin collection.\n",
      " |\n",
      " |      Args:\n",
      " |          include_prompt (bool): Whether to include semantic functions in the list.\n",
      " |          include_native (bool): Whether to include native functions in the list.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of KernelFunctionMetadata objects in the collection.\n",
      " |\n",
      " |  get_list_of_function_metadata_filters(self, filters: dict[typing.Literal['excluded_plugins', 'included_plugins', 'excluded_functions', 'included_functions'], list[str]]) -> list['KernelFunctionMetadata']\n",
      " |      Get a list of Kernel Function Metadata based on filters.\n",
      " |\n",
      " |      Args:\n",
      " |          filters (dict[str, list[str]]): The filters to apply to the function list.\n",
      " |              The keys are:\n",
      " |                  - included_plugins: A list of plugin names to include.\n",
      " |                  - excluded_plugins: A list of plugin names to exclude.\n",
      " |                  - included_functions: A list of function names to include.\n",
      " |                  - excluded_functions: A list of function names to exclude.\n",
      " |              The included and excluded parameters are mutually exclusive.\n",
      " |              The function names are checked against the fully qualified name of a function.\n",
      " |\n",
      " |      Returns:\n",
      " |          list[KernelFunctionMetadata]: The list of Kernel Function Metadata that match the filters.\n",
      " |\n",
      " |  get_plugin(self, plugin_name: str) -> 'KernelPlugin'\n",
      " |      Get a plugin by name.\n",
      " |\n",
      " |      Args:\n",
      " |          plugin_name (str): The name of the plugin\n",
      " |\n",
      " |      Returns:\n",
      " |          KernelPlugin: The plugin\n",
      " |\n",
      " |      Raises:\n",
      " |          KernelPluginNotFoundError: If the plugin is not found\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from semantic_kernel.functions.kernel_function_extension.KernelFunctionExtension:\n",
      " |\n",
      " |  rewrite_plugins(plugins: semantic_kernel.functions.kernel_plugin.KernelPlugin | list[semantic_kernel.functions.kernel_plugin.KernelPlugin] | dict[str, semantic_kernel.functions.kernel_plugin.KernelPlugin] | None = None) -> dict[str, semantic_kernel.functions.kernel_plugin.KernelPlugin]\n",
      " |      Rewrite plugins to a dictionary.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from semantic_kernel.services.kernel_services_extension.KernelServicesExtension:\n",
      " |\n",
      " |  add_service(self, service: semantic_kernel.services.ai_service_client_base.AIServiceClientBase, overwrite: bool = False) -> None\n",
      " |      Add a single service to the Kernel.\n",
      " |\n",
      " |      Args:\n",
      " |          service (AIServiceClientBase): The service to add.\n",
      " |          overwrite (bool, optional): Whether to overwrite the service if it already exists. Defaults to False.\n",
      " |\n",
      " |  get_prompt_execution_settings_from_service_id(self, service_id: str, type: type[~AI_SERVICE_CLIENT_TYPE] | None = None) -> semantic_kernel.connectors.ai.prompt_execution_settings.PromptExecutionSettings\n",
      " |      Get the specific request settings from the service, instantiated with the service_id and ai_model_id.\n",
      " |\n",
      " |  get_service(self, service_id: str | None = None, type: type[~AI_SERVICE_CLIENT_TYPE] | tuple[type[~AI_SERVICE_CLIENT_TYPE], ...] | None = None) -> semantic_kernel.services.ai_service_client_base.AIServiceClientBase\n",
      " |      Get a service by service_id and type.\n",
      " |\n",
      " |      Type is optional and when not supplied, no checks are done.\n",
      " |      Type should be\n",
      " |          TextCompletionClientBase, ChatCompletionClientBase, EmbeddingGeneratorBase\n",
      " |          or a subclass of one.\n",
      " |          You can also check for multiple types in one go,\n",
      " |          by using a tuple: (TextCompletionClientBase, ChatCompletionClientBase).\n",
      " |\n",
      " |      If type and service_id are both None, the first service is returned.\n",
      " |\n",
      " |      Args:\n",
      " |          service_id (str | None): The service id,\n",
      " |              if None, the default service is returned or the first service is returned.\n",
      " |          type (Type[AI_SERVICE_CLIENT_TYPE] | tuple[type[AI_SERVICE_CLIENT_TYPE], ...] | None):\n",
      " |              The type of the service, if None, no checks are done on service type.\n",
      " |\n",
      " |      Returns:\n",
      " |          AIServiceClientBase: The service, should be a class derived from AIServiceClientBase.\n",
      " |\n",
      " |      Raises:\n",
      " |          KernelServiceNotFoundError: If no service is found that matches the type or id.\n",
      " |\n",
      " |  get_services_by_type(self, type: type[~AI_SERVICE_CLIENT_TYPE] | tuple[type[~AI_SERVICE_CLIENT_TYPE], ...] | None) -> dict[str, semantic_kernel.services.ai_service_client_base.AIServiceClientBase]\n",
      " |      Get all services of a specific type.\n",
      " |\n",
      " |  remove_all_services(self) -> None\n",
      " |      Removes the services from the Kernel, does not delete them.\n",
      " |\n",
      " |  remove_service(self, service_id: str) -> None\n",
      " |      Delete a single service from the Kernel.\n",
      " |\n",
      " |  select_ai_service(self, function: 'KernelFunction', arguments: 'KernelArguments') -> tuple[semantic_kernel.services.ai_service_client_base.AIServiceClientBase, semantic_kernel.connectors.ai.prompt_execution_settings.PromptExecutionSettings]\n",
      " |      Uses the AI service selector to select a service for the function.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from semantic_kernel.services.kernel_services_extension.KernelServicesExtension:\n",
      " |\n",
      " |  rewrite_services(services: Union[~AI_SERVICE_CLIENT_TYPE, list[~AI_SERVICE_CLIENT_TYPE], dict[str, ~AI_SERVICE_CLIENT_TYPE], NoneType] = None) -> dict[str, ~AI_SERVICE_CLIENT_TYPE]\n",
      " |      Rewrite services to a dictionary.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from semantic_kernel.kernel_pydantic.KernelBaseModel:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |\n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |\n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |\n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |\n",
      " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]' from pydantic._internal._repr.Representation\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |\n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      " |\n",
      " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |\n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |\n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |\n",
      " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |\n",
      " |      If you need `include` or `exclude`, use:\n",
      " |\n",
      " |      ```py\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |\n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |\n",
      " |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      " |\n",
      " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      " |\n",
      " |  model_copy(self, *, update: 'dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/serialization/#model_copy\n",
      " |\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |\n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |\n",
      " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/serialization/#modelmodel_dump\n",
      " |\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |\n",
      " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/serialization/#modelmodel_dump_json\n",
      " |\n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |\n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |\n",
      " |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
      " |\n",
      " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema'\n",
      " |      Hook into generating the model's CoreSchema.\n",
      " |\n",
      " |      Args:\n",
      " |          source: The class we are generating a schema for.\n",
      " |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      " |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      " |\n",
      " |      Returns:\n",
      " |          A `pydantic-core` `CoreSchema`.\n",
      " |\n",
      " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue'\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |\n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |\n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called.\n",
      " |\n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |\n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by pydantic.\n",
      " |\n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |\n",
      " |  from_orm(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |\n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |\n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |\n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |\n",
      " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]'\n",
      " |      Generates a JSON schema for a model class.\n",
      " |\n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |\n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |\n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |\n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |\n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |\n",
      " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'dict[str, Any] | None' = None) -> 'bool | None'\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |\n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |\n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |\n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |\n",
      " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Validate a pydantic model instance.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |\n",
      " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/json/#json-parsing\n",
      " |\n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |\n",
      " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |\n",
      " |  parse_obj(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |\n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
      " |\n",
      " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str'\n",
      " |\n",
      " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
      " |\n",
      " |  validate(value: 'Any') -> 'Self'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |\n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |\n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __pydantic_extra__\n",
      " |\n",
      " |  __pydantic_fields_set__\n",
      " |\n",
      " |  __pydantic_private__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __pydantic_root_model__ = False\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import semantic_kernel as sk\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "print(help(request=kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__repr__', '__repr_args__', '__repr_name__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_inner_auto_function_invoke_handler', '_iter', 'add_embedding_to_object', 'add_filter', 'add_function', 'add_functions', 'add_plugin', 'add_plugin_from_openai', 'add_plugin_from_openapi', 'add_plugins', 'add_service', 'ai_service_selector', 'auto_function_invocation_filters', 'construct', 'construct_call_stack', 'copy', 'dict', 'filter', 'from_orm', 'function_invocation_filters', 'get_full_list_of_function_metadata', 'get_function', 'get_function_from_fully_qualified_function_name', 'get_list_of_function_metadata', 'get_list_of_function_metadata_bool', 'get_list_of_function_metadata_filters', 'get_plugin', 'get_prompt_execution_settings_from_service_id', 'get_service', 'get_services_by_type', 'invoke', 'invoke_function_call', 'invoke_prompt', 'invoke_prompt_stream', 'invoke_stream', 'json', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'plugins', 'prompt_rendering_filters', 'remove_all_services', 'remove_filter', 'remove_service', 'retry_mechanism', 'rewrite_plugins', 'rewrite_services', 'schema', 'schema_json', 'select_ai_service', 'services', 'update_forward_refs', 'validate']\n"
     ]
    }
   ],
   "source": [
    "print(dir(kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "gpt35 = OpenAIChatCompletion(ai_model_id=\"gpt-3.5-turbo\")\n",
    "gpt4o = OpenAIChatCompletion(ai_model_id=\"gpt-4o\")\n",
    "\n",
    "kernel.add_service(service=gpt35)\n",
    "kernel.add_service(service=gpt4o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To send the prompt to the service, we need to use a method called **`create_semantic_function`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a simple prompt\n",
    "\n",
    "1. **Load the prompt in a string variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Finish the following knock-knock joke. Knock, knock. Who's there? Dishes. Dishes who?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a *`function`* by using the **`add_function`** method of the kernel. The *`function_name`* and *`plugin_name`* parameters of the function are required but are not used, so you can give your function and plugin whatever name you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_function = kernel.add_function(\n",
    "    function_name=\"subrata\",\n",
    "    plugin_name=\"sample\",\n",
    "    prompt=prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Call the function. Note that all invocation methods are *`Asynchronous`*, so we need to use *`await`* to wait for their return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dishes the police, open up!\n"
     ]
    }
   ],
   "source": [
    "response = await kernel.invoke(function=prompt_function, request=prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dishes the police, open up!\n"
     ]
    }
   ],
   "source": [
    "# Entire Code in One Place\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "# Initialize the Kernel\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "# Define Connectors\n",
    "gpt35 = OpenAIChatCompletion(ai_model_id=\"gpt-3.5-turbo\")\n",
    "gpt4o = OpenAIChatCompletion(ai_model_id=\"gpt-4o\")\n",
    "\n",
    "# Add the connectors to the kernel's service\n",
    "kernel.add_service(service=gpt35)\n",
    "kernel.add_service(service=gpt4o)\n",
    "\n",
    "# Write a prompt\n",
    "prompt = \"Finish the following knock-knock joke. Knock, knock. Who's there? Dishes. Dishes who?\"\n",
    "\n",
    "# Create a function (semantic) using add_function method of the kernel\n",
    "prompt_function = kernel.add_function(\n",
    "    function_name=\"Subrata\",\n",
    "    plugin_name=\"Mondal\",\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "# Invoke the created function\n",
    "response = await kernel.invoke(\n",
    "    function=prompt_function,\n",
    "    request=prompt,\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Functions and Native Functions\n",
    "\n",
    "- **Semantic Functions:** are functions that connect to AI Services (LLM) to perform a task. The default parameter for all semantic functions is called *`input`*.\n",
    "\n",
    "- **Native Functions:** are regular functions written in your programming language (Python).\n",
    "\n",
    "\n",
    "The reason to differentiate a Native Function from any other Regular Function in your code is that the native funcion will have additional attributes that will tell the Kernel what it does.\n",
    "\n",
    "After loading a native function into the Kernel, you can use it in chains that combine native and semantic functions. On top of it, **`Semantic Kernel Planner`** can use the function when creating a plan to achieve a user goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Semantic Function in Python\n",
    "\n",
    "1. **Adding parameter to the Semantic Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dishes the police! Open up!\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "\n",
    "args = KernelArguments(input=\"Boo\")\n",
    "\n",
    "response = await kernel.invoke(\n",
    "    function=prompt_function,\n",
    "    request=prompt,\n",
    "    arguments=args,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Native Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Native Functions are created in the same language our application is using like Python. \n",
    "\n",
    "In Python, the **Native functions need to be inside a `Class`**. The Class used to be called **`Skill`** and now it's called **`Plugin`**.\n",
    "\n",
    "- **`Plugin:`** is just a *`collection of functions`*. You **cannot mix both the `Native` and `Semantic Function` in the same Plugin/Skill**. For, example we'll create a **`ShowManager` Plugin**.\n",
    "\n",
    "**How to create a Native Function?**\n",
    "\n",
    "To create a Native Function, we will use **`@kernel_function` decorator**. And the decorator must contain fields for **`description`** and **`name`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
    "\n",
    "\n",
    "class ShowManager:\n",
    "    @kernel_function(\n",
    "        description=\"Randomly choose among a theme for a joke.\",\n",
    "        name=\"random_theme\",\n",
    "    )\n",
    "    def random_theme(self) -> str:\n",
    "        themes: list[str] = [\"Boo\", \"Dishes\", \"Art\", \"Needle\", \"Tank\", \"Police\"]\n",
    "        theme: str = random.choice(seq=themes)\n",
    "        return theme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to load the *Plugin (ShowManager)* and all its *functions* in the Kernel, we use the **`add_plugin`** method of the Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_choice_kernel_plugin = kernel.add_plugin(\n",
    "    plugin=ShowManager(), plugin_name=\"ShowManager\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call the Native Function from a *Plugin (ShowManager)*, simply put the name of the method within brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Art\n"
     ]
    }
   ],
   "source": [
    "response = await kernel.invoke(\n",
    "    theme_choice_kernel_plugin[\"random_theme\"],\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-tutorial-lffBnLp0-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
